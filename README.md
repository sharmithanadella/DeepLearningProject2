Hereâ€™s a suggested **README.md** for your `dl-project2.ipynb` notebook. You can drop this file alongside your notebook in the same repo or folder:

```markdown
# DeepMochi â€“ DL ProjectÂ 2: Parameterâ€‘Efficient Text Classification with LoRA

This repository contains **DL ProjectÂ 2** for SpringÂ 2025: a parameterâ€‘efficient fineâ€‘tuning of RoBERTaâ€‘base on the AGNEWS dataset using **LoRA** (Lowâ€‘Rank Adaptation).

---

## ğŸ“‹ Table of Contents

- [Project Overview](#project-overview)  
- [Notebook Structure](#notebook-structure)  
- [Getting Started](#getting-started)  
  - [Requirements](#requirements)  
  - [Installation](#installation)  
- [Usage](#usage)  
- [Results](#results)  
- [Citations](#citations)  

---

## ğŸš€ Project Overview

In this project, we demonstrate how to fineâ€‘tune a large pretrained transformer (**RoBERTaâ€‘base**, 125M parameters) on the AGNEWS newsâ€classification task while updating **fewer than one million parameters** by injecting lowâ€‘rank adapters (LoRA) into the attention layers.

Key highlights:

- **Dataset:** AGNEWS (4â€way classification of news articles)  
- **Base Model:** RoBERTaâ€‘base (all original weights frozen)  
- **Parameterâ€Efficient Fineâ€Tuning:** LoRA adapters (rank _r_Â =Â 8, Î±Â =Â 16) applied to query/value projections  
- **Performance:** 94.22Â % test accuracy with only 888,580 trainable parameters  

---

## ğŸ“‘ Notebook Structure

The Jupyter notebook `dl-project2.ipynb` is organized into the following sections:

1. **Environment Setup**  
   - Import libraries (PyTorch, Transformers, PEFT, Datasets, etc.)  
   - GPU detection and seed setting  

2. **Data Loading & Preprocessing**  
   - Download and load AGNEWS via `datasets`  
   - Tokenization with `AutoTokenizer` (max lengthÂ =Â 128)  
   - Formatting for PyTorch  

3. **Model Definition**  
   - Load pretrained `roberta-base` model for sequence classification  
   - Configure LoRA adapters (`LoraConfig`) on query/value modules  
   - Wrap model with `get_peft_model`  

4. **Training Setup**  
   - Define `TrainingArguments` (learning rate, batch sizes, epochs, etc.)  
   - Implement `compute_metrics` for accuracy  
   - Initialize `Trainer`  

5. **Training & Evaluation**  
   - Run `trainer.train()`  
   - Evaluate on test set (`trainer.evaluate()`)  
   - Print final accuracy  

6. **Inference & Submission**  
   - Load and preprocess unlabeled test data from pickle  
   - Batchâ€‘wise prediction loop  
   - Save predictions to `submission.csv`  

---

## ğŸ› ï¸ Getting Started

### Requirements

- Python **3.8+**  
- PyTorch **1.13+**  
- Hugging Face **transformers**, **datasets**, **peft**  
- **scikit-learn** (for accuracy)  

You can install the core dependencies with:

```bash
pip install torch transformers datasets peft scikit-learn
```

### Installation

1. Clone this repo (or download the notebook):

   ```bash
   git clone https://github.com/yourusername/deepmochi-dl-project2.git
   cd deepmochi-dl-project2
   ```

2. (Optional) Create and activate a virtual environment:

   ```bash
   python3 -m venv venv
   source venv/bin/activate
   ```

3. Install requirements:

   ```bash
   pip install -r requirements.txt
   ```

> **Note:** A `requirements.txt` with pinned versions can be added:

```
torch>=1.13
transformers>=4.30
datasets>=2.10
peft>=0.4
scikit-learn>=1.2
```

---

## â–¶ï¸ Usage

1. Open the notebook:

   ```bash
   jupyter lab dl-project2.ipynb
   ```

2. Run cells in order.  
   - Make sure you have access to a GPU for faster training.  
   - The Kaggle data loader cells at the top can be removed if running locallyâ€”just ensure the AGNEWS dataset downloads via the hub.

3. After training, `submission.csv` will be created in the working directory.

---

## ğŸ“Š Results

- **Test Accuracy:** 94.22Â %  
- **Trainable Parameters:** 888,580 (<Â 1Â % of RoBERTaâ€‘base)  
- **LoRA Rank (r):** 8  
- **Scaling Factor (Î±):** 16  

See the notebookâ€™s **Results** and **Model Architecture** sections for detailed tables and diagrams.

---

## ğŸ“š Citations

If you use this code, please cite:

- Edward J.Â Hu et al., â€œLoRA: Low-Rank Adaptation of Large Language Models,â€ *arXiv:2106.09685*, 2021.  
- Tim Dettmers et al., â€œPEFT: State-of-the-Art Parameter-Efficient Fine-Tuning Methods,â€ *arXiv:2402.12354*, 2024.  

---

**DeepMochi** | SpringÂ 2025 | Sharmitha Vijayan (sv2877), TseÂ HengÂ Hsueh (th3448), SanjanaÂ Kosuru (sk11528)  
```

Feel free to adjust paths or add any projectâ€‘specific notes!
